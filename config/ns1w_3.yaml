default: &DEFAULT

  #General
  # For computing compression
  n_params_baseline: None
  verbose: True
  arch: tfno


  #Distributed computing
  distributed:
    use_distributed: False #False in darcy
    wireup_info: 'mpi'
    wireup_store: 'tcp'
    model_parallel_size: 2
    seed: 666


  # FNO related
  tfno:
    n_modes: [8,48,48] #for t,x
    data_channels: None #will be determined by pde-onfo, see config_arch in main_code #1(pde)+1(dim:postion)  (i.e. in_channels, but should use the name data_ here (model_dispatcher L55))
    hidden_channels: 28
    projection_channels: 64
    n_layers: 4
    domain_padding: [0.125,0,0] #None #0.078125 #None
    domain_padding_mode: 'one-sided' #symmetric
    fft_norm: 'forward'
    norm: 'group_norm'
    skip: 'linear'
    implementation: 'factorized'
    separable: 0
    preactivation: 0
    
    use_mlp: 1
    mlp:
        expansion: 0.5
        dropout: 0

    factorization: None #'cp' if use factoriz #None
    rank: 1 #1.0
    fixed_rank_modes: None
    dropout: 0.0
    tensor_lasso_penalty: 0.0
    joint_factorization: False
    fno_block_precision: 'full' # or 'half', 'mixed'
    stabilizer: None # or 'tanh'

    check_mem: 0 #if , print cuda mem
    save_mem: 0 #True # use peak-mem reduction or not
    no_splt: 1 # (default=False) when check_mem=0, if no_split, will override all the split below and no loop-separation is conducted

    block_dvc: [0,0,0,0] #[c,c,c,0] where to save checkpoints (output of i-th block). c: cpu, i(int): cuda:i
                        #block_dvc is suggested to be 0(device_compute). No benefit if it is not 0, but only slower
    block_dvc_y3: [1,1,0,0] # where to save chkpt (x_fno+x_skip_fno)
    block_dvc_y0: [1,1,0,0] # where to save chkpt (x_fno+x_skip_mlp)
    block_mlp_split: [9,9,9,9] # tensor_shape[2]=36
    block_fft_split: [4,4,4,4] #[4,4,4,4]

    layer_dvc: [0,0] #lifting/projection layer: where to save checkpoints
    layer_split: 32 #32 # split dim into pieces


  # Optimizer
  opt:
    n_epochs: 1800 #1000
    renew_opt: False
    learning_rate: 8e-4 # 6e-4 for btz=1, possibly /2 for btz=4(=2^2) 
    training_loss: [h1,pde]
    train_loss_func: [rel,0]
    hk_weight: [1] # [1-st, 2-nd..]-order derivative's weight (no square); recommend [0.8,0.7]
    h_k: 1 #H^k loss, k=1,2
    sep: 1

    pino3:
      res: 256 
      data_batch: 1 # K128 in the code:number of data batches per pde batch ; 
      early: 1 # use data loss when idx%grad_acml<early (early/grad_acml used)

      lam_pde: 2 #i.e. correspond to  #lam of pde; scaling based on average integral?(handled in loss fn)  1 for btz=1
      lam_data: 0.8 # will be decreased  (lam0)
      begin: 3 #epoch>=begin: starting using pde loss () var: epoch_pde
      end: 600 #var:pde_inc_stop
      gamma: 1.2 #increase pde loss weight; var: pde_inc_rate
      period: 100 #every period (increase lambda) var: pde_inc_time
      
      lr_inc: 1 # every period_epoch, increase lr by

      step_end: 10 #after pino3.end(no more reducing data weight): scheduler.step_size change into this
      gm_end: 0.9 # and gamma change into this one

      # pde_smp_bch: 80 #num of pde loss each epoch; use -1 if don't want to (code)continue early in for loop; 4*8+5*8 (grad_acml=8;first 32-with DNS; last 40: no DNS)
      change_sch_epc: -1 # epoch that change scheduler into following:
      new_gm: 0.7 #larger gm, step in latter increasing lambda_pde period
      new_stp: 400
    use_pde: [0] 
    pde_loss_method: t_fc_x_f # t_fc_x_f or t_fd_x_f
    loss_type: 'sum' #'mean' or sum
    weight_decay: 1e-4 #1e-4 # 1e-4
    amp_autocast: False

    scheduler_T_max: 500 # For cosine only, typically take n_epochs
    scheduler_patience: 5 # For ReduceLROnPlateau only
    scheduler: 'StepLR' # Or 'CosineAnnealingLR' OR 'ReduceLROnPlateau'
    step_size: 60 #60
    gamma: 0.9 #0.5

    check_mem: 0
    gradient_accumulation: 8
    quick_save: 10 #2 #3 # ave model every 'quick_save' epoch (qs-1,2qs-1,...); 0 for not saving.

  
  # Dataset related
  data:
    batch_size: [6,6] #[4,6] for 4090
    test_resolutions: ['256test','pde_train'] ##only used as wandb plot label
    test_batch_sizes: [8,8] #
    num_data: [384,1536,16,32] #[384,4608,16,32] #[384,4608,16,32]
    resolution: [256,pde,256,pde] 
    t_start: [0,0,0,0]  #phy time
    t_interval: [1,1,2,1] 
    train_tag: [1,1,0,2] # 1:train; 0: test; 2: test on train set
    

    t_predict: 0.5  #phy time
    traj_for_train: [11,39] #100 in total for N=16, 10 for n=128
    
    positional_encoding: True
    encode_input: False
    encode_output: False

    t_step_min: 32
    repeat: 1

  # Patching
  patching:
    levels: 0
    padding: 0
    stitching: False


  # Weights and biases
  wandb:
    log: True
    name: None # If None, config will be used but you can override it here
    group: '' 
    project: # add project name here
    entity:  # add your username here
    sweep: False
    log_output: True
    log_test_interval: 1
    pde: kf
    pino: 1
    loss: pde #cgs, frs, pde: the loss function to three stages
    savemd: 0 #True
    model_use: 0 
    model_use_type: 2 # Model after stage 1: supervised loss with CGS data. Model after stage 2: supervised loss with FRS data. Model after stage 3: pde loss
