default: &DEFAULT

  #General
  # For computing compression
  n_params_baseline: None
  verbose: True
  # arch: 'tfno2d'
  arch: tfno


  #Distributed computing
  distributed:
    use_distributed: False #False in darcy
    wireup_info: 'mpi'
    wireup_store: 'tcp'
    model_parallel_size: 2
    seed: 666

  # FNO related
  tfno:
    n_modes: [8,48,48] #for t,x
    data_channels: None #will be determined by pde-onfo, see config_arch in main_code #1(pde)+1(dim:postion)  (i.e. in_channels, but should use the name data_ here (model_dispatcher L55))
    hidden_channels: 28
    projection_channels: 64
    n_layers: 4
    domain_padding: [0.125,0,0] #None #0.078125 #None
    domain_padding_mode: 'one-sided' #symmetric
    fft_norm: 'forward'
    norm: 'group_norm'
    skip: 'linear'
    implementation: 'factorized'
    separable: 0
    preactivation: 0
    
    use_mlp: 1
    mlp:
        expansion: 0.5
        dropout: 0

    factorization: None
    rank: 1.0
    fixed_rank_modes: None
    dropout: 0.0
    tensor_lasso_penalty: 0.0
    joint_factorization: False
    fno_block_precision: 'full' # or 'half', 'mixed'
    stabilizer: None # or 'tanh'

    check_mem: 0 #if true, print cuda mem
    save_mem: 0 #True # use peak-mem reduction or not
    no_splt: 1 # (default=False) when save_mem=0, if no_split, will override all the split below and no loop-separation is conducted

    block_dvc: [0,c,c,0] # where to save checkpoints (output of i-th block). c: cpu, i(int): cuda:i
    block_dvc_y3: [c,c,c,c] # where to save chkpt (x_fno+x_skip_fno)
    block_dvc_y0: [c,c,c,c] # where to save chkpt (x_fno+x_skip_mlp)
    block_mlp_split: [4,4,4,4] # the i-th element refers to the number of for loops when splitting tensor computation in mlp within the i-th fno block
    block_fft_split: [1,1,1,1] # the i-th element refers to the number of for loops when splitting tensor computation in fft within the i-th fno block

    layer_dvc: [0,0] #lifting/projection layer: where to save checkpoints. c: cpu, i(int): cuda:i
    layer_split: 32 #32 # split dim into pieces


  
  # Dataset related
  data:

    t_predict: 0.5  #phy time

    positional_encoding: True
    encode_input: False
    encode_output: False

    t_step_min: 32
    repeat: 1
  plot:
    
    n_traj: 100
    btz_traj: 20 # split the computation with for loops to fit in cuda memory: btz_traj each iteration.
    t_start_save: 320 # physical time
    t_run: 500 # #physical time 
    repeat_ini: 32
    res: 48
    res_ini: 48 
    

  # Patching
  patching:
    levels: 0
    padding: 0
    stitching: False

  # Weights and biases
  wandb:
    log: True
    name: None # If None, config will be used but you can override it here
    group: '' 
    project: # add project name here
    entity:  # add your username here
    sweep: False
    log_output: True
    log_test_interval: 1
    pde: kf
    pino: 1
    loss: pde #cgs, frs, pde: the loss function to three stages
    savemd: 0 #True
    model_use: 0 
    model_use_type: 3 # Model after stage 1: supervised loss with CGS data. Model after stage 2: supervised loss with FRS data. Model after stage 3: pde loss
